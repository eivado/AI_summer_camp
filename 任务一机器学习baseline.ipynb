{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ad1d7949-0ea7-4a33-af4d-2beb6f9be245",
   "metadata": {},
   "source": [
    "# 基于机器学习-逻辑回归实现的完整代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ac79dc9-234a-4976-8508-5ab2bdcbd9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'al', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 导入pandas用于读取表格数据\n",
    "import pandas as pd\n",
    "\n",
    "# 导入BOW（词袋模型），可以选择将CountVectorizer替换为TfidfVectorizer（TF-IDF（词频-逆文档频率）），注意上下文要同时修改，亲测后者效果更佳\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 导入LogisticRegression回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 过滤警告消息\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# 读取数据集\n",
    "train = pd.read_csv('~/autodl-tmp/gitdir/datawhale/基于论文摘要的文本分类与关键词抽取挑战赛公开数据/train.csv')\n",
    "train['title'] = train['title'].fillna('')\n",
    "train['abstract'] = train['abstract'].fillna('')\n",
    "\n",
    "test = pd.read_csv('~/autodl-tmp/gitdir/datawhale/基于论文摘要的文本分类与关键词抽取挑战赛测试集B/testB.csv')\n",
    "test['title'] = test['title'].fillna('')\n",
    "test['abstract'] = test['abstract'].fillna('')\n",
    "\n",
    "# 提取文本特征，生成训练集与测试集\n",
    "train['text'] = train['title'].fillna('') + ' ' +  train['author'].fillna('') + ' ' + train['abstract'].fillna('')+ ' ' + train['Keywords'].fillna('')\n",
    "test['text'] = test['title'].fillna('') + ' ' +  test['author'].fillna('') + ' ' + test['abstract'].fillna('')\n",
    "\n",
    "# 添加停用词分析\n",
    "stops =[i.strip() for i in open(r'/root/autodl-tmp/gitdir/datawhale/stop.txt',encoding='utf-8').readlines()]\n",
    "\n",
    "#第一次用BOW算法，没有停止词，讯飞提交结果是0.671\n",
    "#vector = CountVectorizer().fit(train['text'])\n",
    "#第二次用停用词和TF_IDF算法，讯飞提交结果是0.759，增加了0.9个百分点\n",
    "vector = TfidfVectorizer(stop_words=stops).fit(train['text'])\n",
    "train_vector = vector.transform(train['text'])\n",
    "test_vector = vector.transform(test['text'])\n",
    "\n",
    "# 引入模型\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 开始训练，这里可以考虑修改默认的batch_size与epoch来取得更好的效果\n",
    "model.fit(train_vector, train['label'])\n",
    "\n",
    "# 利用模型对测试集label标签进行预测\n",
    "test['label'] = model.predict(test_vector)\n",
    "test['Keywords'] = test['title'].fillna('')\n",
    "# 生成任务一推测结果\n",
    "test[['uuid', 'Keywords', 'label']].to_csv('submit_task1.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fb0f926-346c-470d-a3fd-5923a967841b",
   "metadata": {},
   "source": [
    "下面具体的分步骤讲解"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddf5d0cb-ac07-46f4-812b-bcd305dd2775",
   "metadata": {},
   "source": [
    "导入我们本次Baseline代码所需的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4584904-726a-420c-b41d-550c8f55a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fdf135-42b3-4bc7-b663-8414741d8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae64504-64b0-494d-a144-cb24284580e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 相关库\n",
    "# 导入pandas用于读取表格数据\n",
    "import pandas as pd\n",
    "\n",
    "# 导入BOW（词袋模型），可以选择将CountVectorizer替换为TfidfVectorizer（TF-IDF（词频-逆文档频率）），注意上下文要同时修改，亲测后者效果更佳\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 导入LogisticRegression回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 过滤警告消息\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408098cf-2f53-4234-9b13-7527b43f076b",
   "metadata": {},
   "source": [
    "特征提取是机器学习任务中的一个重要步骤。我们将训练数据的每一个维度称为一个特征，例如，如果我们想要基于二手车的品牌、价格、行驶里程数三个变量来预测二手车的价格，则品牌、价格、行驶里程数为该任务的三个特征。所谓特征提取，即从训练数据的特征集合中创建新的特征子集的过程。提取出来的特征子集特征数一般少于等于原特征数，但能够更好地表征训练数据的情况，使用提取出的特征子集能够取得更好的预测效果。对于 NLP、CV 任务，我们通常需要将文本、图像特征提取为计算机可以处理的数值向量特征。我们一般可以使用 sklearn 库中的 feature_extraction 包来实现文本与图片的特征提取。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41981632-98f9-48cf-966e-65d3ef056150",
   "metadata": {},
   "source": [
    "在 NLP 任务中，特征提取一般需要将自然语言文本转化为数值向量表示，常见的方法包括基于 TF-IDF（词频-逆文档频率）提取或基于 BOW（词袋模型）提取等，两种方法均在 sklearn.feature_extraction 包中有所实现。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e47998f6-8e4b-4362-9d83-e9e13bea37c4",
   "metadata": {},
   "source": [
    "# 基于 TF-IDF 提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8498b63-c0e7-4147-b314-f86534567a64",
   "metadata": {},
   "source": [
    "TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，其中，TF 指 term frequence，即词频，指某个词在文章中出现次数与文章总次数的比值；IDF 指 inverse document frequence，即逆文档频率，指包含某个词的文档数占语料库总文档数的比例。例如，假设语料库为 {\"今天 天气 很好\",\"今天 心情 很 不好\", \"明天 天气 不好\"}，每一个句子为一个文档，则“今天”的 TF 和 IDF 分别为：\n",
    "\n",
    "看文档-此处缺失\n",
    "每个词最终的 IF-IDF 即为 TF 值乘以 IDF 值。计算出每个词的 TF-IDF 值后，使用 TF-IDF 计算得到的数值向量替代原文本即可实现基于 TF-IDF 的文本特征提取。\n",
    "\n",
    "我们可以使用 sklearn.feature_extraction.text 中的 TfidfVectorizer 类来简单实现文档基于 TF-IDF 的特征提取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed56b54b-6240-4ac9-bf76-16758df7bfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入该类\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 假设我们已从本地读取数据为 DataFrame 类型，并已经过基本预处理，data 为已处理的 DataFrame 数据\n",
    "# 实例化一个 TfidfVectorizer 对象，并使用 fit 方法来拟合数据\n",
    "text=[\"hello world\"]\n",
    "vector = TfidfVectorizer().fit(text)\n",
    "\n",
    "# 拟合之后，调用 transform 方法即可得到提取后的特征数据\n",
    "train_vector = vector.transform(text)\n",
    "train_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162e5f6-7416-44a3-bcfd-0449223885f5",
   "metadata": {},
   "source": [
    "基于 BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc93914-358d-49da-9573-529077894117",
   "metadata": {},
   "source": [
    "BOW（Bag of Words）是一种常用的文本表示方法，其基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子（因为里面装的都是词汇，所以称为词袋，Bag of words即因此而来），然后看这个袋子里装的都是些什么词汇，将其分类。具体而言，词袋模型表示一个文本，首先会维护一个词库，词库里维护了每一个词到一个数值向量的映射关系。例如，最简单的映射关系是独热编码，假设词库里一共有四个词，今天、天气、很、不好，那么独热编码会将四个词分别编码为：\n",
    "\n",
    "今天——（1,0,0,0）\n",
    "\n",
    "天气——（0,1,0,0）\n",
    "\n",
    "很 ——（0,0,1,0）\n",
    "\n",
    "不好——（0,0,0,1）\n",
    "\n",
    "而使用词袋模型，就会将上述这句话编码为：\n",
    "\n",
    "BOW(Sentence）=Embedding(今天)+Embedding(天气)+Embedding(很)+Embedding(不好)=(1,1,1,1）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69423bc6-194f-4f2a-9e26-32bf3b152a78",
   "metadata": {},
   "source": [
    "我们一般使用 sklearn.feature_extraction.text 中的 CountVectorizer 类来简单实现文档基于频数统计的 BOW 特征提取，其主要方法与 TfidfVectorizer 的主要使用方法一致："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "307d3c71-7fdb-49e5-83ea-740097d799b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入该类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 假设我们已从本地读取数据为 DataFrame 类型，并已经过基本预处理，data 为已处理的 DataFrame 数据\n",
    "# 实例化一个 CountVectorizer 对象，并使用 fit 方法来拟合数据\n",
    "text=[\"hello\"]\n",
    "vector = CountVectorizer().fit(text)\n",
    "\n",
    "# 拟合之后，调用 transform 方法即可得到提取后的特征数据\n",
    "train_vector = vector.transform(text)\n",
    "train_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d6190-a898-441e-a06c-cb29fd0cebff",
   "metadata": {},
   "source": [
    "# 停用词"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7776d57-b3ef-4a5d-93ca-f9c859d5e3d0",
   "metadata": {},
   "source": [
    "停用词(Stop Words)是自然语言处理领域的一个重要工具，通常被用来提升文本特征的质量，或者降低文本特征的维度。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb9bc2a4-935d-4641-a641-2e77f6ad8b2e",
   "metadata": {},
   "source": [
    "当我们在使用TF-IDF或者BOW模型来表示文本时，总会遇到一些问题。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c50d80a8-5913-46f7-b9af-0e814c4074f6",
   "metadata": {},
   "source": [
    "在特定的NLP任务中，一些词语不能提供有价值的信息作用、可以忽略。这种情况在生活里也非常普遍。以本次学习任务为例，我们希望医学类的词语在特征提取时被突出，对于不是医学类词语的数据就应该考虑让他在特征提取时被淡化，同时一些日常生活中使用频率过高而普遍出现的词语，我们也应该选择忽略这些词语，以防对我们的特征提取产生干扰\n",
    "\n",
    "举个例子，我们依然以讲解BOW模型时举的这个例子介绍：\n",
    "\n",
    "BOW(Sentence）=Embedding(今天)+Embedding(天气)+Embedding(很)+Embedding(不好)=(1,1,1,1）\n",
    "当我们需要对这句话进行情感分类时，我们就需要突出它的情感特征，也就是我们希望**‘不好’**这个词在经过BOW模型编码后的值能够大一点\n",
    "\n",
    "但是如果我们不使用停用词，那么“今天天气很好还是不好”这句话经过BOW模型编码后的值就会与上面这句话的编码高度相似，从而严重影响模型判断的结果。\n",
    "\n",
    "那么我们如何使用停用词解决这个问题呢，理想一点，我们将除了情感元素的词语全部停用，也就是编码时不考虑，仅保留情感词语，也就是判断句子中好这个词出现的多还是少，很明显好这个词出现的多那情感很显然就是正向的。\n",
    "\n",
    "对于本次任务而言，日常生活中出现的词语可能都对模型分类很难有太大帮助例如（or，again，and），下面贴出我收集的一些对任务可能没有那么多帮助的词语文件。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77e23bf3-c0d3-4ae9-93bb-b43575b87df9",
   "metadata": {},
   "source": [
    "如何使用这个文件呢，利用下面所示方法读取该文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb2bca0-07f0-433e-a288-880800baf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops =[i.strip() for i in open(r'/root/autodl-tmp/gitdir/datawhale/stop.txt',encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b2ad648-2a80-4a81-87b0-f20798266e7d",
   "metadata": {},
   "source": [
    "读取这个文件后在使用CountVectorizer()方法时指定stop_words参数为stops就可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e5bbf5-df1b-48f0-bf97-74eee8281ac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstops\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1334\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \n\u001b[1;32m   1321\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1334\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1289\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1291\u001b[0m         )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer(stop_words=stops).fit(text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "047df83a-8be0-43cd-a373-0bf10b74a0db",
   "metadata": {},
   "source": [
    "# 划分数据集"
   ]
  },
  {
   "cell_type": "raw",
   "id": "340724f0-aee6-4e8f-9011-72767e30166e",
   "metadata": {},
   "source": [
    "在机器学习任务中，我们一般会有三个数据集：训练集、验证集、预测集。训练集为我们训练模型的拟合数据，是我们前期提供给模型的输入；验证集一般是我们自行划分出来验证模型效果以选取最优超参组合的数据集；测试集是最后检验模型效果的数据集。例如在本期竞赛任务中，比赛方提供的 test.csv 就是预测集，我们最终的任务是建立一个模型在预测集上实现较准确的预测。但是预测集一般会限制预测次数，例如在本期比赛中，每人每天仅能提交三次，但是我们知道，机器学习模型一般有很多超参数，为了选取最优的超参组合，我们一般需要多次对模型进行验证，即提供一部分数据让已训练好的模型进行预测，来找到预测准确度最高的模型。\n",
    "\n",
    "因此，我们一般会将比赛方提供的训练集也就是 train.csv 进行划分，划分为训练集和验证集。我们会使用划分出来的训练集进行模型的拟合和训练，而使用划分出来的验证集验证不同参数及不同模型的效果，来找到最优的模型及参数再在比赛方提供的预测集上预测最终结果。\n",
    "\n",
    "划分数据集的方法有很多，基本原则是同分布采样。即我们划分出来的验证集和训练集应当是同分布的，以免验证不准确（事实上，最终的预测集也应当和训练集、验证集同分布）。此处我们介绍交叉验证，即对于一个样本总量为 T 的数据集，我们一般随机采样 10%~20%（也就是 0.1T~0.2T 的样本数）作为验证集，而取其他的数据为训练集。如要了解更多的划分方法，可查阅该博客：https://​blog​.csdn​.net​/hcxddd​/article​/details​/119698879。我们可以使用 sklearn.model_selection 中的 train_test_split 函数便捷实现数据集的划分："
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccda472e-ed85-4ef7-8452-c0a6a86d0fe1",
   "metadata": {},
   "source": [
    "baseline中并没有划分验证集，你也可以自行划分验证集来观察训练中的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf54134d-1d11-461d-89ff-1c3d1c88d6a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 该函数将会根据给定比例将数据集划分为训练集与验证集\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m trian_data, eval_data \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mdata\u001b[49m, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 该函数将会根据给定比例将数据集划分为训练集与验证集\n",
    "trian_data, eval_data = train_test_split(data, test_size = 0.2)\n",
    "# 参数 data 为总数据集，可以是 DataFrame 类型\n",
    "# 参数 test_size 为划分验证集的占比，此处选择0.2，即划分20%样本作为验证集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100eb61-bd44-43a2-a547-e4866b8bdb36",
   "metadata": {},
   "source": [
    "# 选择机器学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ae986-7cf3-4602-9f19-025b35daccfd",
   "metadata": {},
   "source": [
    "我们可以选择多种机器学习模型来拟合训练数据，不同的业务场景、不同的训练数据往往最优的模型也不同。常见的模型包括线性模型、逻辑回归、决策树、支持向量机、集成模型、神经网络等。想要深入学习各种机器学习模型的同学，推荐学习《西瓜书》或《统计学习方法》。\n",
    "\n",
    "Sklearn 封装了多种机器学习模型，常见的模型都可以在 sklearn 中找到，sklearn 根据模型的类别组织在不同的包中，此处介绍几个常用包：\n",
    "\n",
    "sklearn.linear_model：线性模型，如线性回归、逻辑回归、岭回归等\n",
    "sklearn.tree：树模型，一般为决策树\n",
    "sklearn.neighbors：最近邻模型，常见如 K 近邻算法\n",
    "sklearn.svm：支持向量机\n",
    "sklearn.ensemble：集成模型，如 AdaBoost、GBDT等\n",
    "本案例中，我们使用简单但拟合效果较好的逻辑回归模型作为 Baseline 的模型。此处简要介绍其原理。\n",
    "\n",
    "逻辑回归模型，即 Logistic Regression，实则为一个线性分类器，通过 Logistic 函数(或 Sigmoid 函数)，将数据特征映射到0～1区间的一个概率值（样本属于正例的可能性），通过与 0.5 的比对得出数据所属的分类。逻辑回归的数学表达式为："
   ]
  },
  {
   "cell_type": "raw",
   "id": "65553e63-9ac2-4636-9c77-5f934b826cee",
   "metadata": {},
   "source": [
    "逻辑回归模型简单、可并行化、可解释性强，同时也往往能够取得不错的效果，是较为通用的模型。\n",
    "\n",
    "我们可以使用 sklearn.linear_model.LogisticRegression 来调用已实现的逻辑回归模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b061026-9b4b-4064-8d59-9a1ab21166b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 可以在初始化时控制超参的取值，此处使用默认值，具体参数可以查阅官方文档\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 开始训练，这里可以考虑修改默认的batch_size与epoch来取得更好的效果\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 此处的 train_vector 是已经经过特征提取的训练数据\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_vector, \u001b[43mtrain\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 利用模型对测试集label标签进行预测，此处的 test_vector 同样是已经经过特征提取的测试数据\u001b[39;00m\n\u001b[1;32m     10\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_vector)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# 引入模型\n",
    "model = LogisticRegression()\n",
    "# 可以在初始化时控制超参的取值，此处使用默认值，具体参数可以查阅官方文档\n",
    "\n",
    "# 开始训练，这里可以考虑修改默认的batch_size与epoch来取得更好的效果\n",
    "# 此处的 train_vector 是已经经过特征提取的训练数据\n",
    "model.fit(train_vector, train['label'])\n",
    "\n",
    "# 利用模型对测试集label标签进行预测，此处的 test_vector 同样是已经经过特征提取的测试数据\n",
    "test['label'] = model.predict(test_vector)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8a33f51-cae3-4ed1-b98f-17767c1cfa7d",
   "metadata": {},
   "source": [
    "事实上，sklearn 提供的多种机器学习模型都封装成了类似的类，绝大部分使用方法均和上述一致，即先实例化一个模型对象，再使用 fit 函数拟合训练数据，最后使用 predict 函数预测测试数据即可。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4480e274-531e-41ba-8ab8-7592ce69b108",
   "metadata": {},
   "source": [
    "# 数据探索"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33672063-8ed0-48ef-a088-e3e6254a8cb1",
   "metadata": {},
   "source": [
    "数据探索性分析，是通过了解数据集，了解变量间的相互关系以及变量与预测值之间的关系，对已有数据在尽量少的先验假设下通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律的一种数据分析方法，从而帮助我们后期更好地进行特征工程和建立模型，是机器学习中十分重要的一步。\n",
    "\n",
    "本次baseline实践中我们使用pandas来读取数据以及数据探索"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a6613d8-5c2c-45e7-ba4c-20f91d706f00",
   "metadata": {},
   "source": [
    "# 使用pandas读取数据"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee5e1e70-8f9c-4f4c-88c1-97141e33e4f8",
   "metadata": {},
   "source": [
    "在这部分内容里我们利用**pd.read_csv（)方法对赛题数据进行读取，pd.read_csv（)**参数为需要读取的数据地址，读取后返回一个DataFrame 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6462e14-c15e-409e-9706-74aec8122aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('/root/autodl-tmp/gitdir/datawhale/基于论文摘要的文本分类与关键词抽取挑战赛公开数据/train.csv')\n",
    "train['title'] = train['title'].fillna('')\n",
    "train['abstract'] = train['abstract'].fillna('')\n",
    "\n",
    "test = pd.read_csv('/root/autodl-tmp/gitdir/datawhale/基于论文摘要的文本分类与关键词抽取挑战赛测试集B/testB.csv')\n",
    "test['title'] = test['title'].fillna('')\n",
    "test['abstract'] = test['abstract'].fillna('')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1300e65e-54e9-4f9e-89a7-4d08f422cfb0",
   "metadata": {},
   "source": [
    "通过pandas提供的一些方法，我们可以在本地快速查看数据的一些特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6ce3052-a89b-4d26-bfb5-84ee97345c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    6000.000000\n",
      "mean       95.045333\n",
      "std        33.454553\n",
      "min         8.000000\n",
      "25%        72.000000\n",
      "50%        90.000000\n",
      "75%       114.000000\n",
      "max       285.000000\n",
      "Name: title, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train['title'].apply(len).describe())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07d6aae5-f093-49bf-80d8-c5259dace54d",
   "metadata": {},
   "source": [
    "观察输出发现数据长度平均值在1620左右"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa1d693c-262c-470d-b6e3-d8c477f6937a",
   "metadata": {},
   "source": [
    "通过DataFrame.value_counts()方法查看数据数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df7f96e4-58d7-4194-87f1-c2ab25ed2918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    3079\n",
      "1    2921\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faecce5c-9fb2-4d5c-9397-b9c259787e4f",
   "metadata": {},
   "source": [
    "观察输出发现0,1标签分布的比较均匀，也就是说我们不必担心数据分布不均而发生过拟合，保证模型的泛化能力"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22588338-4f47-4186-b3c2-4acbf7090851",
   "metadata": {},
   "source": [
    "数据清洗"
   ]
  },
  {
   "cell_type": "raw",
   "id": "839d66c5-f9b9-4e5c-a536-ae05ab0044d0",
   "metadata": {},
   "source": [
    "数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。俗话说：garbage in, garbage out。分析完数据后，特征工程前，必不可少的步骤是对数据清洗。\n",
    "\n",
    "数据清洗的作用是利用有关技术如数理统计、数据挖掘或预定义的清理规则将脏数据转化为满足数据质量要求的数据。主要包括缺失值处理、异常值处理、数据分桶、特征归一化/标准化等流程。\n",
    "\n",
    "同时由于表格中存在较多列，我们将这些列的重要内容组合在一起生成一个新的列方便训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15f5c81b-f09c-45aa-9d9f-5c91ffa96157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 提取文本特征，生成训练集与测试集\n",
    "train['text'] = train['title'].fillna('') + ' ' +  train['author'].fillna('') + ' ' + train['abstract'].fillna('')+ ' ' + train['Keywords'].fillna('')\n",
    "test['text'] = test['title'].fillna('') + ' ' +  test['author'].fillna('') + ' ' + test['abstract'].fillna('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdc50a-a0de-4867-a856-8bc9195e39b8",
   "metadata": {},
   "source": [
    "特征工程"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f721faef-7e1b-4e43-89c4-23dd82d22967",
   "metadata": {},
   "source": [
    "特征工程指的是把原始数据转变为模型训练数据的过程，目的是获取更好的训练数据特征。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a90cdea-4bc0-4cc5-967a-b72f3591b250",
   "metadata": {},
   "source": [
    "这里我们选择使用BOW将文本转换为向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88f63b18-27fb-46a6-b973-5cea4072788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征工程\n",
    "vector = CountVectorizer().fit(train['text'])\n",
    "train_vector = vector.transform(train['text'])\n",
    "test_vector = vector.transform(test['text'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fe230a7-fc45-4be6-82f3-8e1ac97c0b64",
   "metadata": {},
   "source": [
    "# 模型训练与验证"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82a5cd7f-c097-4487-9867-19cae88b78a6",
   "metadata": {},
   "source": [
    "特征工程也好，数据清洗也罢，都是为最终的模型来服务的，模型的建立和调参决定了最终的结果。模型的选择决定结果的上限， 如何更好的去达到模型上限取决于模型的调参。\n",
    "\n",
    "建模的过程需要我们对常见的线性模型、非线性模型有基础的了解。模型构建完成后，需要掌握一定的模型性能验证的方法和技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f372643-3eb9-452d-beb8-ebf1683bbd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 开始训练，这里可以考虑修改默认的batch_size与epoch来取得更好的效果\n",
    "model.fit(train_vector, train['label'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f712daa-b532-48bf-8dc2-98abb6fc77c1",
   "metadata": {},
   "source": [
    "# 结果输出"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a5e451f-4aad-493a-b729-4444f16c4816",
   "metadata": {},
   "source": [
    "# 提交结果需要符合提交样例结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d113ddca-b70e-47ff-adfb-78cc074a8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用模型对测试集label标签进行预测\n",
    "test['label'] = model.predict(test_vector)\n",
    "test['Keywords'] = test['title'].fillna('')\n",
    "# 生成任务一推测结果\n",
    "test[['uuid', 'Keywords', 'label']].to_csv('submit_task1.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bca380b3-a6fd-47fb-a1e4-2a2296d89c84",
   "metadata": {},
   "source": [
    "完整代码如下："
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
