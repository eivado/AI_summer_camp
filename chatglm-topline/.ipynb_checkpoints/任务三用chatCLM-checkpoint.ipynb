{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec1ca89-9068-4a95-8601-f4c16838e63b",
   "metadata": {},
   "source": [
    ">靡不有初，鲜克有终。学习本就是一个克服困难、迎难而上的过程。当你看到了此仓库，并且决定要成功运行本代码，提交submit.csv。\n",
    "那么我希望你遇到到困难之后不要放弃，把这块硬骨头啃下来。鲜克有终，你要做那个有始有终的人！\n",
    "\n",
    "教程介绍： https://vj6fpcxa05.feishu.cn/docx/DIged2HfIojIYlxWP9Hc2x0UnVd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490c309-ad7d-4620-8b0c-9414d0f73966",
   "metadata": {},
   "source": [
    "# 大模型介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ff531-f8d8-43eb-990f-0c6ac1753c4f",
   "metadata": {},
   "source": [
    "> 自 20 世纪 50 年代图灵测试提出以来，人们始终在探索机器处理语言智能的能力。语言本质上是一个错综复杂的人类表达系统，受到语法规则的约束。因此，开发能够理解和精通语言的强大 AI 算法面临着巨大挑战。过去二十年，语言建模方法被广泛用于语言理解和生成，包括统计语言模型和神经语言模型。\n",
    "近些年，研究人员通过在大规模语料库上预训练 Transformer 模型产生了预训练语言模型（PLMs），并在解决各类 NLP 任务上展现出了强大的能力。并且研究人员发现模型缩放可以带来性能提升，因此他们通过将模型规模增大进一步研究缩放的效果。有趣的是，当参数规模超过一定水平时，这个更大的语言模型实现了显著的性能提升，并出现了小模型中不存在的能力，比如上下文学习。为了区别于 PLM，这类模型被称为大型语言模型（LLMs）。\n",
    "从 2019 年的谷歌 T5 到 OpenAI GPT 系列，参数量爆炸的大模型不断涌现。可以说，LLMs 的研究在学界和业界都得到了很大的推进，尤其去年 11 月底对话大模型 ChatGPT 的出现更是引起了社会各界的广泛关注。LLMs 的技术进展对整个 AI 社区产生了重要影响，并将彻底改变人们开发和使用 AI 算法的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d2d75-8f74-453c-b47b-307b9ed9ca5e",
   "metadata": {},
   "source": [
    "# 大模型是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d4130-ef31-4dc5-b7ac-2fe2dbe1df07",
   "metadata": {},
   "source": [
    "通常，大型语言模型（LLM）是指包含数千亿（或更多）参数的语言模型，这些参数是在大量文本数据上训练的，例如模型 GPT-3、PaLM、Galactica 和 LLaMA。具体来说，LLM 建立在 Transformer 架构之上，其中多头注意力层堆叠在一个非常深的神经网络中。现有的 LLM 主要采用与小语言模型类似的模型架构（即 Transformer）和预训练目标（即语言建模）。作为主要区别，LLM 在很大程度上扩展了模型大小、预训练数据和总计算量（扩大倍数）。他们可以更好地理解自然语言，并根据给定的上下文（例如 prompt）生成高质量的文本。这种容量改进可以用标度律进行部分地描述，其中性能大致遵循模型大小的大幅增加而增加。然而根据标度律，某些能力（例如，上下文学习）是不可预测的，只有当模型大小超过某个水平时才能观察到。\n",
    "像chatgpt、claude AI、文心一言、讯飞星火和通义千问一样的，这种模型通过对大量的文本数据进行学习，可以理解和生成人类的自然语言，甚至可以编程、写数学题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92390ea-f12e-4ba1-8206-646a8b5411e4",
   "metadata": {},
   "source": [
    "# 大模型的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc94dc-0c60-4884-893d-d8bf0685842d",
   "metadata": {},
   "source": [
    "大语言模型的语言生成的原理叫做自回归模型，是统计上一种处理时间序列的方法。例如现在有一个句子：“我早上去了星巴克”，将其拆分为“我”、“早上”、“去了”、“星巴克”这四个词（我们叫做 token）。大语言模型是这样去学习的：第一个单词是<sos>，输入到模型中。经过了一个Transformer模块后，它输出希望被训练成第一个字，也就是“我”。\n",
    "在第二个位置，它的输入是“我”，它的输出是“早上”，第三个位置输入是“早上”，输出是“去了”，这样不断地一轮一轮迭代，每个细胞能够根据我现在的这个单词，去预测下一个单词。最后这个模型就能够学到，“我早上去了星巴克”的下一句话应该是“星巴克里的咖啡很好喝”，它认为生成“星巴克里的咖啡很好喝”比“我明天要去乘飞机”这样一句话合理得多。\n",
    "自回归模型的关键是根据你前面已经出现过的内容，来推测它的下一个字，下一句话应该是怎样生成的，在这样不断的迭代过程中，它就能学会如何去生成一句话、一个段落，以及一篇文章。\n",
    "总的来说，LLM可以理解为大规模的语言模型。从历史的角度来看，前面说的BERT和GPT并没有达到足够大的规模。直到GPT-2、GPT-3出现了，它们才达到了较大的量级。大家发现语言模型爆炸式增长，从一个细胞长成一个脑子，这种程度上的增长才带来了LLMs。所以我们一般理解LLMs，这个语言模型规模大到了至少到GPT-1或2阶段，它的参数量能够突破1亿或者10亿阶段才能称之为大模型。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c70063e-15fc-44f9-880f-e6c67aeb9bbc",
   "metadata": {},
   "source": [
    "LLM 应用概况。不同颜色表示不同的模型适应程度，包括预训练、微调、提示策略、评估。\n",
    "聊天机器人、计算生物学、计算机编程（https://codeium.com/）、创意工作、知识工作、法律（https://github.com/PKU-YuanGroup/ChatLaw）、医学（https://github.com/X-D-Lab/Sunsimiao）、推理、机器人和具身智能体、社会科学和心理学、生成合成数据等多个不同领域有不同程度的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705b9ab-6b7d-44b7-8b9a-d288f5db97f4",
   "metadata": {},
   "source": [
    "# 大模型是如何训练的？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50bec1-1f18-4b87-a074-cf73abf7a766",
   "metadata": {},
   "source": [
    "OpenAI 开发 ChatGPT 的三个主要步骤：大尺寸预训练+指令微调+RLHF\n",
    "1. 大尺寸预训练：在这个阶段，模型在大规模的文本数据集上进行预训练。这是一个非监督学习的过程，模型需要预测在给定的文本序列中下一个词是什么。预训练的目标是让模型学会理解和生成人类语言的基本模式。\n",
    "2. 指令微调：在预训练之后，模型会在一个更小但专门针对特定任务的数据集上进行微调。这个数据集通常由人工生成，包含了模型需要学会的任务的特定指令。例如，如果我们想要让模型学会如何进行数学计算，我们就需要提供一些包含数学问题和对应解答的数据。\n",
    "3. RLHF（Reinforcement Learning from Human Feedback）：这是一个强化学习过程，模型会根据人类提供的反馈进行学习和优化。首先，我们会收集一些模型的预测结果，并让人类评估这些结果的好坏。然后，我们会使用这些评估结果作为奖励，训练模型优化其预测性能。通过这种方式，模型可以学会生成更符合人类期望的结果。\n",
    "通过这三个步骤，模型能够在理解和生成人类语言的基础上，更好地完成特定任务，更好地符合人类的期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a1ca0-7581-44cc-9d7c-2a78be7f2804",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c25b0-d127-411b-b070-22bb61f428f4",
   "metadata": {},
   "source": [
    "Prompt顾名思义表示「提示」，比如在高中时代，英语考试中的完形填空，有 4 个提示选项，我们只需要在 4 个里面选择合适的一个作为答案即可。在大语言模型中，提示模型输出我们想要的答案，这里就需要Prompt。（相关课程链接：https://github.com/datawhalechina/prompt-engineering-for-developers）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab584a-de5d-41b8-85c0-242456fea7c1",
   "metadata": {},
   "source": [
    "# 大模型微调介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7728eb1-b76e-4da2-a1e8-c4646ca17f2b",
   "metadata": {},
   "source": [
    "* 什么是大模型微调 *\n",
    "将预训练好的语言模型（LM）在下游任务上进行微调已成为处理 NLP 任务的一种范式。与使用开箱即用的预训练 LLM (例如：零样本推理) 相比，在下游数据集上微调这些预训练 LLM 会带来巨大的性能提升。\n",
    "但是，随着模型变得越来越大，在消费级硬件上对模型进行全部参数的微调（full fine-tuning）变得不可行。\n",
    "此外，为每个下游任务独立存储和部署微调模型变得非常昂贵，因为微调模型（调整模型的所有参数）与原始预训练模型的大小相同。\n",
    "因此，近年来研究者们提出了各种各样的参数高效迁移学习方法（Parameter-efficient Transfer Learning），即固定住Pretrain Language model（PLM）的大部分参数，仅调整模型的一小部分参数来达到与全部参数的微调接近的效果（调整的可以是模型自有的参数，也可以是额外加入的一些参数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a4612-c45d-43e0-923d-c659fe2ef57c",
   "metadata": {},
   "source": [
    "# 微调方法介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb6cfe-d537-4337-ade2-df68e2b7f1ba",
   "metadata": {},
   "source": [
    "- 参考：https://blog.csdn.net/sinat_39620217/article/details/131751780"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e797e69-ea3b-48e1-9a03-8a224ad6eb28",
   "metadata": {},
   "source": [
    "LoRA（Low-Rank Adaptation）：它的基本思想是对模型的一部分进行低秩适应，即找到并优化那些对特定任务最重要的部分。也就是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型微调类似的效果。这种方法可以有效地减少模型的复杂性，同时保持模型在特定任务上的表现。对 Transformer 的每一层结构都采用 LoRA 微调的方式，最终可以使得模型微调参数量大大减少。当部署到生产环境中时，只需要计算和存储W=W0+BA，并像往常一样执行推理。与其它方法相比，没有额外的延迟，因为不需要附加更多的层。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b0254f3-5f35-482e-8e66-659a30e164ec",
   "metadata": {},
   "source": [
    "2. P-tuning v2：P-tuning v2是一种新的微调方法，也是chatglm官方仓库使用的微调方法。它的基本思想是在原有的大型语言模型上添加一些新的参数，这些新的参数可以帮助模型更好地理解和处理特定的任务。具体来说，P-tuning v2首先确定模型在处理特定任务时需要的新的参数（这些参数通常是模型的某些特性或功能），然后在模型中添加这些新的参数，以提高模型在特定任务上的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f250c8-360f-45fd-9c59-a89e592e8938",
   "metadata": {},
   "source": [
    "* P-tuning v2 官方介绍：https://www.bilibili.com/video/BV1fd4y1Z7Y5 *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307b2d6-444c-4366-9dc2-a9b8405d97ef",
   "metadata": {},
   "source": [
    "# 大模型微调都可以做什么呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdaea0e-7d40-4996-ba5b-3cc88f7a1d3c",
   "metadata": {},
   "source": [
    "- 训练垂领域的大模型，比如北大团队开发的ChatLaw法律大模型，还有医疗领域大模型和银行大模型等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565edb07-695f-4632-b097-96cf6ea069fb",
   "metadata": {},
   "source": [
    "- 也有我们小组训练的个性化AI模型---huanhuan-chat（Chat-嬛嬛）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04d004-0c5e-4ba8-b99a-21ae07fd12d0",
   "metadata": {},
   "source": [
    "# 建立数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a87b8-0d73-4fb7-a964-c800e572139d",
   "metadata": {},
   "source": [
    "目前所称的大语言模型，一般指指令微调大模型，即经过预训练——指令微调——人类反馈强化学习全过程训练的大模型，对人类指令具备较强的理解和执行能力，而不是进行简单的文本生成。例如，当你询问“中国的首都是什么？”时，你的指令即你的问题，通用模型将会简单地对你的问题进行最大概率预测地生成，例如回答“中国的首都是什么？这是一个人尽皆知的问题......”而指令微调大模型会理解你的指令，知道你想要模型对你的问题给出答案，从而回答“北京”。ChatGPT 与 GPT-3 就是鲜明的例子。GPT-3 是通用大模型，因此对你的输入只能进行最大概率预测地输出；而 ChatGPT 是指令微调大模型，可以理解并执行你的指令，例如帮助你写代码，帮助你判断问题等。\n",
    "正如其名，指令微调大模型与通用大模型最大的区别在于进行了指令微调，即训练模型理解并执行指令的能力。指令微调一般会给模型特定的指令及执行指令后的输出，要求模型学习遵循指令的能力。这里的指令可以有很多种类型，包括回答问题、续写、创作等。具体而言，指令微调时通常有两个输入，一个是你要求模型执行的指令，另一个是执行该指令所必须的输入。例如，当你要求模型执行指令“续写我的话”，那么你还需要输入该指令中指代的话，例如“今天天气很好，我们去”。而指令微调的输出一般是模型执行该指令后的输出，例如对于上述示例，输出应该为“郊游吧”。\n",
    "本任务中，我们所要对大模型进行的即是指令微调。通过训练模型在特定指令（在本任务中，即要求其判断文献是否医学领域文献）下的指令执行能力，来使用大模型完成本任务。我们也需要按照指令微调的格式进行数据集的建立。使用 LoRA 进行指令微调时，数据集一般有三个元素：instruction，input，output。instruction 即指令，即上文中我们所提到的第一个输入——要求模型执行的指令。input 即上文中我们提到的第二个输入——执行该指令所必须的输入。output 即指令微调的输出。\n",
    "对于各种下游任务，只需要对应去构造特定的指令与输入即可。例如，在本次任务中，我们需要让大模型实现文本分类任务，那我们构造的指令、输入与输出应该为：\n",
    "- instruction：指令，Please judge whether it is a medical field paper according to the given paper title and abstract, output 1 or 0, the following is the paper title and abstract -->\n",
    "- \"-->\":加上一个箭头就是让大模型明白，下次再遇到这种问题，就是我们想让大模型进行二分类任务。\n",
    "- input：prompt。对于这个任务来说那就是 title+abstract+author 拼接成的字符串了。\n",
    "- output：response，即大模型的回答，0 or 1。\n",
    "如果是对于任务二的关键词生成，那么我们应该将指令修改为要求模型根据给定文献的标题、摘要生成关键词，输入不变，输出更改为文献的关键词：\n",
    "- instruction：指令，Please extract keywords from the given paper title and abstract in the text below, separated by commas -->\n",
    "- \"-->\":加上一个箭头就是让大模型明白，下次再遇到这种问题，就是我们想让大模型进行关键词抽取任务。\n",
    "- input：prompt。对于这个任务来说那就是 title+abstract+author 拼接成的字符串了。\n",
    "- output：response，即大模型的回答，具体抽取出的关键词。\n",
    "再例如在我们的项目 Chat-嬛嬛中，我们仅需要模型能够对用户输入进行模仿甄嬛语气的回答，因此可以将指令退化为简单的问答，所以在构造数据集时，我们的指令为对话集的上文，输出是甄嬛的回答，不需要额外的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4bb20-0593-4bc5-bdb3-b79adb49d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"instruction\": \"小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"嘘——都说许愿说破是不灵的。\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518dd41d-7432-4dee-9ba4-d0389917c3f8",
   "metadata": {},
   "source": [
    "- 最后在data目录下的data_info.json文件内添加数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a6449-01fb-4636-b2dd-36f23d81655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"数据集名称\": {\n",
    "    \"file_name\": \"data目录下数据集文件的名称\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11634b-6a70-4dda-83fc-71ae76f42991",
   "metadata": {},
   "source": [
    "上面这些都没听懂也没关系，在xfg-paper的仓库我们把这次微调的脚本、环境和数据集完全准备好啦~，接下来就可以开始你的第一次大模型微调之旅！！！\n",
    "微调，跑起来！（code）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9d0a4-5cf0-4422-a6fa-5f32e42008f1",
   "metadata": {},
   "source": [
    "- 首先阅读文档：阿里云机器学习Pai-DSW服务器部署教程 ，配置好阿里云DSW服务器。\n",
    "- clone微调脚本：git clone https://github.com/KMnO4-zx/xfg-paper.git\n",
    "- 下载chatglm2-6b模型：git clone https://huggingface.co/THUDM/chatglm2-6b，这行命令可能会失败，没关系多试几次！模型下载时间需要十几分钟，大家耐心等待哦~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21678eaa-d215-41ce-8769-639225ad0381",
   "metadata": {},
   "source": [
    "（如果卡在中间也可以在确保git clone https://huggingface.co/THUDM/chatglm2-6b执行成功但无反应后，进入''chatglm2-6b''文件下，用如下命令下载模型【慎用】）\n",
    "此命令只用于下载模型文件，py文件不会下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a135e-41e8-449b-92cf-615153450dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://cloud.tsinghua.edu.cn/seafhttp/files/f3e22aa1-83d1-4f83-917e-cf0d19ad550f/pytorch_model-00001-of-00007.bin https://cloud.tsinghua.edu.cn/seafhttp/files/0b6a3645-0fb7-4931-812e-46bd2e8d8325/pytorch_model-00002-of-00007.bin https://cloud.tsinghua.edu.cn/seafhttp/files/f61456cb-5283-4529-a7bc-400355140e4b/pytorch_model-00003-of-00007.bin https://cloud.tsinghua.edu.cn/seafhttp/files/1a1f68c5-1a7d-489a-8f16-8432a099d782/pytorch_model-00004-of-00007.bin https://cloud.tsinghua.edu.cn/seafhttp/files/6357afba-bb40-4348-bc33-f08c1fcc2936/pytorch_model-00005-of-00007.bin  https://cloud.tsinghua.edu.cn/seafhttp/files/ebec3ae2-5ae4-432c-83e4-df4b147026bb/pytorch_model-00006-of-00007.bin https://cloud.tsinghua.edu.cn/seafhttp/files/7d1aab8a-d255-47f7-87c9-4c0593379ee9/pytorch_model-00007-of-00007.bin https://cloud.tsinghua.edu.cn/seafhttp/files/4daca87e-0d34-4cff-bd43-5a40fcdf4ab1/tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b2b65-0b91-437f-994c-fcfa00e0696f",
   "metadata": {},
   "source": [
    "进入目录安装环境：cd ./xfg-paper；pip install -r requirements.txt\n",
    "- 将脚本中的model_name_or_path更换为你本地的chatglm2-6b模型路径，然后运行脚本：sh xfg_train.sh\n",
    "- 微调过程大概需要两个小时（我使用阿里云A10-24G运行了两个小时左右），微调过程需要15G的显存，推荐使用16G、24G显存的显卡，比如3090，4090等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b6a08-d0c4-4eb8-813f-c803d0d5f12e",
   "metadata": {},
   "source": [
    "- 当然，我们已经把训练好的lora权重放在了仓库里，您可以直接运行下面的代码。\n",
    "- 也可以选择运行仓库内的jupyter notebook文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c2522-4be6-4cb0-b03f-6dc0817a8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
    "    --model_name_or_path chatglm2-6b \\ 本地模型的目录\n",
    "    --stage sft \\ 微调方法\n",
    "    --use_v2 \\ 使用glm2模型微调，默认值true\n",
    "    --do_train \\ 是否训练，默认值true\n",
    "    --dataset paper_label \\ 数据集名字\n",
    "    --finetuning_type lora \\ \n",
    "    --lora_rank 8 \\  LoRA 微调中的秩大小\n",
    "    --output_dir ./output/label_xfg \\ 输出lora权重存放目录\n",
    "    --per_device_train_batch_size 4 \\ 用于训练的批处理大小\n",
    "    --gradient_accumulation_steps 4 \\ 梯度累加次数\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\ 日志输出间隔\n",
    "    --save_steps 1000 \\ 断点保存间隔\n",
    "    --learning_rate 5e-5 \\ 学习率\n",
    "    --num_train_epochs 4.0 \\ 训练轮数\n",
    "    --fp16 是否使用 fp16 半精度 默认值：False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255d08e-6f72-46a5-8dbe-7e085755eef7",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbbb82-8250-4d97-9871-ff17b93277da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 pandas 库，用于数据处理和分析\n",
    "import pandas as pd\n",
    "# 读取训练集和测试集\n",
    "train_df = pd.read_csv('./csv_data/train.csv')\n",
    "testB_df = pd.read_csv('./csv_data/testB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f58aaa-8cea-443b-83b5-9a37e64ea463",
   "metadata": {},
   "source": [
    "制作数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234b2dc-6b6d-4a98-aa2c-e9e6bb7d2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空列表来存储数据样本\n",
    "res = []\n",
    "\n",
    "# 遍历训练数据的每一行\n",
    "for i in range(len(train_df)):\n",
    "    # 获取当前行的数据\n",
    "    paper_item = train_df.loc[i]\n",
    "    # 创建一个字典，包含指令、输入和输出信息\n",
    "    tmp = {\n",
    "    \"instruction\": \"Please judge whether it is a medical field paper according to the given paper title and abstract, output 1 or 0, the following is the paper title and abstract -->\",\n",
    "    \"input\": f\"title:{paper_item[1]},abstract:{paper_item[3]}\",\n",
    "    \"output\": str(paper_item[5])\n",
    "  }\n",
    "    # 将字典添加到结果列表中\n",
    "    res.append(tmp)\n",
    "\n",
    "# 导入json包，用于保存数据集\n",
    "import json\n",
    "# 将制作好的数据集保存到data目录下\n",
    "with open('./data/paper_label.json', mode='w', encoding='utf-8') as f:\n",
    "    json.dump(res, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0e1bc-f447-498a-ba50-0b310b7adde9",
   "metadata": {},
   "source": [
    "修改data_info\n",
    "- 修改data目录下data_info文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ea5d1-0548-43f8-9072-2eb2a0d4f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"paper_label\": {\n",
    "    \"file_name\": \"paper_label.json\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef951c-3c4e-47f5-98c6-6282cbd48818",
   "metadata": {},
   "source": [
    "加载训练好的LoRA权重，进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8634d-15f5-418f-a63e-3ec93c620883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModel, GenerationConfig, AutoModelForCausalLM\n",
    "\n",
    "# 定义预训练模型的路径\n",
    "model_path = \"../chatglm2-6b\"\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# 加载 label lora权重\n",
    "model = PeftModel.from_pretrained(model, './output/label_xfg').half()\n",
    "model = model.eval()\n",
    "# 使用加载的模型和分词器进行聊天，生成回复\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab7d41-2948-43a5-89f3-5e6855e64a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "\n",
    "def predict(text):\n",
    "    # 使用加载的模型和分词器进行聊天，生成回复\n",
    "    response, history = model.chat(tokenizer, f\"Please judge whether it is a medical field paper according to the given paper title and abstract, output 1 or 0, the following is the paper title and abstract -->{text}\", history=[],\n",
    "    temperature=0.01)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422fab1-1ca4-4cc9-8caf-db931324a73b",
   "metadata": {},
   "source": [
    "制作submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a430e-3273-40dc-9624-24388aa8f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测测试集\n",
    "# 导入tqdm包，在预测过程中有个进度条\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 建立一个label列表，用于存储预测结果\n",
    "label = []\n",
    "\n",
    "# 遍历测试集中的每一条样本\n",
    "for i in tqdm(range(len(testB_df))):\n",
    "    # 测试集中的每一条样本\n",
    "    test_item = testB_df.loc[i]\n",
    "    # 构建预测函数的输入：prompt\n",
    "    test_input = f\"title:{test_item[1]},author:{test_item[2]},abstract:{test_item[3]}\"\n",
    "    # 将预测结果存入lable列表\n",
    "    label.append(int(predict(test_input)))\n",
    "\n",
    "# 把label列表赋予testB_df\n",
    "testB_df['label'] = label\n",
    "# task1虽然只需要label，但需要有一个keywords列，用个随意的字符串代替\n",
    "testB_df['Keywords'] = ['tmp' for _ in range(2000)]\n",
    "# 制作submit，提交submit\n",
    "submit = testB_df[['uuid', 'Keywords', 'label']]\n",
    "submit.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb072172-7346-4285-a8bd-64f198046526",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally，提交submit.csv，鲜克有终。\n",
    "submit.csv提交之后评分应该在1.000左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3b9d5-794b-40fb-9c67-1bf0d0e48029",
   "metadata": {},
   "outputs": [],
   "source": [
    "相关参考：\n",
    "https://mp.weixin.qq.com/s/XMQLp4-fRm5K6QHbbBH_tw\n",
    "https://zhuanlan.zhihu.com/p/635152813\n",
    "https://zhuanlan.zhihu.com/p/619083290\n",
    "https://www.zhihu.com/question/585091993\n",
    "https://arxiv.org/abs/2307.10169\n",
    "https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2303.18223"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
