{
 "cells": [
  {
   "cell_type": "raw",
   "id": "017200eb-c756-40c9-bc05-3c6c81c8d30b",
   "metadata": {},
   "source": [
    "使用预训练的BERT模型进行建模的思路步骤如下：\n",
    "\n",
    "数据预处理：首先，对文本数据进行预处理，包括文本清洗（如去除特殊字符、标点符号）、分词等操作。可以使用常见的NLP工具包（如NLTK或spaCy）来辅助进行预处理。\n",
    "构建训练所需的dataloader与dataset，构建Dataset类时，需要定义三个方法__init__，__getitem__， __len__，其中__init__方法完成类初始化，__getitem__要求返回返回内容和label，__len__方法返回数据长度\n",
    "构造Dataloader，在其中完成对句子进行编码、填充、组装batch等动作：\n",
    "定义预测模型利用预训练的BERT模型来解决文本二分类任务，我们将使用BERT模型编码中的[CLS]向量来完成二分类任务\n",
    "[CLS]就是classification的意思，可以理解为用于下游的分类任务。\n",
    "\n",
    "主要用于以下两种任务：\n",
    "\n",
    "单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。\n",
    "\n",
    "在模型设计中思路就体现为我们取出文本数据经过向量化后的[CLS]向量，然后经过二分类预测层得到最终的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc907637-44f5-4981-9a80-ce8d3e7f4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
    "self.predictor(outputs)\n",
    "self.predictor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32c49bf2-d1e9-48ad-8425-a0455a70836f",
   "metadata": {},
   "source": [
    "模型训练和评估：使用训练集对选定的机器学习模型进行训练，然后使用测试集进行评估。评估指标可以选择准确率、精确率、召回率、F1值等。\n",
    "调参优化：如果模型效果不理想，可以尝试调整特征提取的参数（如词频阈值、词袋大小等）或机器学习模型的参数，以获得更好的性能。\n",
    "在这个进阶实践中，我们使用深度学习方法，一般会遵循以下流程："
   ]
  },
  {
   "cell_type": "raw",
   "id": "e76dbc23-05fd-4cab-916b-4c107595cf67",
   "metadata": {},
   "source": [
    "数据收集与准备->模型定义->模型训练->模型评估与优化->结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4fbe2-ff82-4265-8625-2ce44a54c95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "008b8764-4fe2-468f-8252-84fdf0e46fa6",
   "metadata": {},
   "source": [
    "导入我们本次Baseline代码所需的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec2683-6df7-4c8d-9fb4-de5b5fdada2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 相关库\n",
    "#导入前置依赖\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 用于加载bert模型的分词器\n",
    "from transformers import AutoTokenizer\n",
    "# 用于加载bert模型\n",
    "from transformers import BertModel\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee53affd-d089-4910-b70e-e23bbdf58cf1",
   "metadata": {},
   "source": [
    "设置全局配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd038ceb-475b-46a4-9570-b2444e951d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# 文本的最大长度\n",
    "text_max_length = 128\n",
    "# 总训练的epochs数，我只是随便定义了个数\n",
    "epochs = 100\n",
    "# 学习率\n",
    "lr = 3e-5\n",
    "# 取多少训练集的数据作为验证集\n",
    "validation_ratio = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 每多少步，打印一次loss\n",
    "log_per_step = 50\n",
    "\n",
    "# 数据集所在位置\n",
    "dataset_dir = Path(\"./基于论文摘要的文本分类与关键词抽取挑战赛公开数据\")\n",
    "os.makedirs(dataset_dir) if not os.path.exists(dataset_dir) else ''\n",
    "\n",
    "# 模型存储路径\n",
    "model_dir = Path(\"./model/bert_checkpoints\")\n",
    "# 如果模型目录不存在，则创建一个\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b698a35-1443-4037-9759-b451037dff6e",
   "metadata": {},
   "source": [
    "数据收集与准备"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7a79b08-60a3-4ad3-8f89-34edd502807c",
   "metadata": {},
   "source": [
    "在赛题主页下载数据，读取数据集，数据预处理（考虑数据扩增）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae173f-3aaa-40ab-a73b-0e6863d618da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集，进行数据处理\n",
    "\n",
    "pd_train_data = pd.read_csv('./基于论文摘要的文本分类与关键词抽取挑战赛公开数据/train.csv')\n",
    "pd_train_data['title'] = pd_train_data['title'].fillna('')\n",
    "pd_train_data['abstract'] = pd_train_data['abstract'].fillna('')\n",
    "\n",
    "test_data = pd.read_csv('./基于论文摘要的文本分类与关键词抽取挑战赛公开数据/testB.csv')\n",
    "test_data['title'] = test_data['title'].fillna('')\n",
    "test_data['abstract'] = test_data['abstract'].fillna('')\n",
    "pd_train_data['text'] = pd_train_data['title'].fillna('') + ' ' +  pd_train_data['author'].fillna('') + ' ' + pd_train_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n",
    "test_data['text'] = test_data['title'].fillna('') + ' ' +  test_data['author'].fillna('') + ' ' + test_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n",
    "test['Keywords'] = test['title'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5a616-b3f1-4a8a-bb27-8da4824a8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集中随机采样测试集\n",
    "validation_data = pd_train_data.sample(frac=validation_ratio)\n",
    "train_data = pd_train_data[~pd_train_data.index.isin(validation_data.index)]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5867242f-779e-4661-963c-419b939f4bcf",
   "metadata": {},
   "source": [
    "# 构建训练所需的dataloader与dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9115ac6-b9f0-4e71-add7-bf433722aba8",
   "metadata": {},
   "source": [
    "# 定义dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb030a-55f7-4884-8880-349b9e492c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Dataset\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        # 拿到对应的数据\n",
    "        if mode == 'train':\n",
    "            self.dataset = train_data\n",
    "        elif mode == 'validation':\n",
    "            self.dataset = validation_data\n",
    "        elif mode == 'test':\n",
    "            # 如果是测试模式，则返回内容和uuid。拿uuid做target主要是方便后面写入结果。\n",
    "            self.dataset = test_data\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode {}\".format(mode))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 取第index条\n",
    "        data = self.dataset.iloc[index]\n",
    "        # 取其内容\n",
    "        text = data['text']\n",
    "        # 根据状态返回内容\n",
    "        if self.mode == 'test':\n",
    "            # 如果是test，将uuid做为target\n",
    "            label = data['uuid']\n",
    "        else:\n",
    "            label = data['label']\n",
    "        # 返回内容和label\n",
    "        return text, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6970b3f-e7cf-4aef-82d0-25ff763dda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset('train')\n",
    "validation_dataset = MyDataset('validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9e893-3b6b-49f9-88bb-3aa6579e2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "941f1701-2a3b-49bd-b54d-0d73849b8e83",
   "metadata": {},
   "source": [
    "构造Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37c33b-71c7-4caa-beb6-ea9a1cdc1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取Bert预训练模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f732534-e9f7-4c10-ae7a-0b53b0903524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#接着构造我们的Dataloader。\n",
    "#我们需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作：\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一个batch的文本句子转成tensor，并组成batch。\n",
    "    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]\n",
    "    :return: 处理后的结果，例如：\n",
    "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
    "             target：[1, 1, 0, ...]\n",
    "    \"\"\"\n",
    "    text, label = zip(*batch)\n",
    "    text, label = list(text), list(label)\n",
    "\n",
    "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
    "    # padding='max_length' 不够长度的进行填充\n",
    "    # truncation=True 长度过长的进行裁剪\n",
    "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
    "\n",
    "    return src, torch.LongTensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c29821-f861-41e3-b830-047e2934c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb2d93-2d58-4303-a51a-96910a7b793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"inputs:\", inputs)\n",
    "print(\"targets:\", targets)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c9ee086-9d97-4348-9e0a-75be19042854",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d568b-1785-4266-827a-c681b0955c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义预测模型，该模型由bert模型加上最后的预测层组成\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 加载bert模型\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', mirror='tuna')\n",
    "\n",
    "        # 最后的预测层\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param src: 分词后的推文数据\n",
    "        \"\"\"\n",
    "\n",
    "        # 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。\n",
    "        # 得到encoder的输出，用最前面[CLS]的输出作为最终线性层的输入\n",
    "        outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # 使用线性层来做最终的预测\n",
    "        return self.predictor(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc328a-57ac-4028-9e50-661655737744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bfa6a4c-4b1b-427e-8434-ac2e2ee06a71",
   "metadata": {},
   "source": [
    "定义出损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c086afc-37ed-4f5c-ae8a-b2818100e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义出损失函数和优化器。这里使用Binary Cross Entropy：\n",
    "criteria = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffcd0fe-d4fb-4d29-83c0-cd2c9608fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于inputs是字典类型的，定义一个辅助函数帮助to(device)\n",
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11b56d1a-ee3d-4473-b526-41f219a869a7",
   "metadata": {},
   "source": [
    "定义验证方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db9057-7058-43a4-be6e-cfba00381f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个验证方法，获取到验证集的精准率和loss\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_correct = 0\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        correct_num = (((outputs >= 0.5).float() * 1).flatten() == targets).sum()\n",
    "        total_correct += correct_num\n",
    "\n",
    "    return total_correct / len(validation_dataset), total_loss / len(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1bdbdf0-c4bc-45aa-89ad-0efcec01afaa",
   "metadata": {},
   "source": [
    "模型训练、评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ebc90-8478-48f0-8b2e-5d67e2e8824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先将模型调成训练模式\n",
    "model.train()\n",
    "\n",
    "# 清空一下cuda缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 定义几个变量，帮助打印loss\n",
    "total_loss = 0.\n",
    "# 记录步数\n",
    "step = 0\n",
    "\n",
    "# 记录在验证集上最好的准确率\n",
    "best_accuracy = 0\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # 从batch中拿到训练数据\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        # 传入模型进行前向传递\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        step += 1\n",
    "\n",
    "        if step % log_per_step == 0:\n",
    "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "        del inputs, targets\n",
    "\n",
    "    # 一个epoch后，使用过验证集进行验证\n",
    "    accuracy, validation_loss = validate()\n",
    "    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))\n",
    "    torch.save(model, model_dir / f\"model_{epoch}.pt\")\n",
    "\n",
    "    # 保存最好的模型\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model, model_dir / f\"model_best.pt\")\n",
    "        best_accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f34f03-a8d8-4e95-9035-50c473841f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载最好的模型，然后进行测试集的预测\n",
    "model = torch.load(model_dir / f\"model_best.pt\")\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d089fed-a508-4b59-bb7b-618d47cbb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset('test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f2a0452-4815-453e-9a79-859819ed5232",
   "metadata": {},
   "source": [
    "结果输出"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34789284-7d8e-408f-beee-542e7633ebe7",
   "metadata": {},
   "source": [
    "提交需要符合提交样例结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64925c-6471-4333-baa4-e97280a7cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for inputs, ids in test_loader:\n",
    "    outputs = model(inputs.to(device))\n",
    "    outputs = (outputs >= 0.5).int().flatten().tolist()\n",
    "    ids = ids.tolist()\n",
    "    results = results + [(id, result) for result, id in zip(outputs, ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8323d-4718-4929-bcc2-dbc7675656f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = [pair[1] for pair in results]\n",
    "test_data['label'] = test_label\n",
    "test_data['Keywords'] = test_data['title'].fillna('')\n",
    "test_data[['uuid', 'Keywords', 'label']].to_csv('submit_task1.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
